import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
import time
from sklearn.linear_model import LinearRegression
import math
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GroupKFold, RepeatedStratifiedKFold, cross_validate, StratifiedShuffleSplit
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
df = pd.read_csv('dataset_test.csv.zip')
df.head()
df.shape
df.info()
train = pd.read_csv("dataset_train_validation.csv.zip")
test = pd.read_csv("dataset_test.csv.zip")
columns =  [col for col in test.columns if col not in ['company', 'age', 'market', 'TARGET']]
submission = test[columns]
solucion = test['TARGET']
def filter_rows_by_values(df, col, values):
    return df[~df[col].isin(values)]

train = filter_rows_by_values(train, "TARGET", ["null"])
def relative_strength_idx(df, n=14):
    close = df['close']
    delta = close.diff()
    delta = delta[1:]
    pricesUp = delta.copy()
    pricesDown = delta.copy()
    pricesUp[pricesUp < 0] = 0
    pricesDown[pricesDown > 0] = 0
    rollUp = pricesUp.rolling(n).mean()
    rollDown = pricesDown.abs().rolling(n).mean()
    rs = rollUp / rollDown
    rsi = 100.0 - (100.0 / (1.0 + rs))
    return rsi
    train['close_lag'] = train['close'].shift(1)
train['RSI'] = relative_strength_idx(train).fillna(0)
train = train.fillna(0)
test['close_lag'] = test['close'].shift(1)
test['RSI'] = relative_strength_idx(test).fillna(0)
test = test.fillna(0)
fraccion_train = 0.7  
fraccion_valid = 1.00 - fraccion_train
train_aleatorio = train.sample(frac=1)
train = train_aleatorio.iloc[:int(fraccion_train * len(train)), :]
validacion = train_aleatorio.iloc[int(fraccion_train * len(train)):, :]
train_X = train[columns]
train_y = train['TARGET']
valid_X = validacion[columns]
valid_y = validacion['TARGET']
pip install lightgbm
import lightgbm as lgb
folds = GroupKFold(n_splits=5)
params = {'objective': 'binary',
          'learning_rate': 0.02,
          "boosting_type": "gbdt",
          "metric": 'precision',
          'n_jobs': -1,
          'min_data_in_leaf': 32,
          'num_leaves': 1024,
          }
for fold_n, (train_index, valid_index) in enumerate(folds.split(train_X, train_y, groups=train['company'])):
    print(f'Fold {fold_n} started at {time.ctime()}')
    X_train, X_valid = train_X[columns].iloc[train_index], train_X[columns].iloc[valid_index]
    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]

    model = lgb.LGBMClassifier(**params, n_estimators=50)
    model.fit(X_train, y_train,
              eval_set=[(X_train, y_train), (X_valid, y_valid)])
feature_importance = pd.DataFrame()
fold_importance = pd.DataFrame()
fold_importance["feature"] = columns
fold_importance["importance"] = model.feature_importances_
fold_importance["fold"] = fold_n + 1
feature_importance = pd.concat([feature_importance, fold_importance], axis=0)
feature_importance["importance"] /= 5
cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
    by="importance", ascending=False)[:50].index
best_features = feature_importance.loc[feature_importance.feature.isin(cols)]
plt.figure(figsize=(16, 12));
sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
plt.title('LGB Features (avg over folds)');
plt.savefig(pathOutput + "BOLSA_feature_importances.png")
print("COMIENZO DE VALIDACIÓN")
score = metrics.mean_absolute_error(valid_y, model.predict(valid_X))
print('CV score: {0:.4f}.'.format(score))
print("FIN DE VALIDACIÓN")
test = filter_rows_by_values(test, "TARGET", ["null"])

submission = test[columns]
solucion = test['TARGET']

prediccion = (model.predict(submission) > 0.5).astype("int32")
submission['TARGET']=prediccion

submission=submission.join(test['company'])
submission.set_index(submission.pop('company'), inplace=True)
submission.reset_index(inplace=True)
submission.to_csv(os.path.join(pathOutput, 'Bolsa_DL_submission.csv'), index=False)

a=solucion
b=prediccion
TP=sum(1 for x,y in zip(a,b) if (x == y and y == 1))
TPandFP=sum(b)
precision= TP / TPandFP
print("TP: ", TP)
print("TP + FP: ", TPandFP)
print("---->>>>>> PRECISION (TP/(TP+FP)) FOR TEST DATASET: {0:.2f}% <<<<<<------".format(precision * 100))

ift_mayoritaria = test[test.TARGET == False]
ift_minoritaria = test[test.TARGET == True]
tasaDesbalanceo = round(ift_mayoritaria.shape[0] / ift_minoritaria.shape[0], 2)
print("Tasa de desbalanceo = " + str(ift_mayoritaria.shape[0]) + "/" + str(
        ift_minoritaria.shape[0]) + " = " + str(tasaDesbalanceo))
print("Tasa de mejora de precisión respecto a random: ",
              round(precision / (1/(1+tasaDesbalanceo)), 2))

from sklearn.metrics import classification_report
print("Informe de metricas:")
print(classification_report(a, b))

print("END")
